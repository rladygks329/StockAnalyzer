"""
í•œêµ­ ì£¼ì‹ ì‹œì¥ ë¶„ì„ ì‹œìŠ¤í…œ - ë©”ì¸ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸

ì‚¬ìš©ë²•:
    python main.py                  # ì¦‰ì‹œ 1íšŒ ë¶„ì„ ì‹¤í–‰
    python main.py --schedule       # ë§¤ì¼ 15:40 ìë™ ì‹¤í–‰ ëª¨ë“œ
    python main.py --date 20260206  # íŠ¹ì • ë‚ ì§œ ë¶„ì„
    streamlit run main.py           # Streamlit ëŒ€ì‹œë³´ë“œ ì‹¤í–‰
"""

import os
import sys
import json
import argparse
import logging
from datetime import datetime
from pathlib import Path

from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.data_collector import StockDataCollector
from src.news_collector import NewsCollector
from src.ai_analyzer import AIAnalyzer
from src.report_generator import ReportGenerator
from src.scheduler import AnalysisScheduler


def setup_logging(level: str = "INFO") -> None:
    """ë¡œê¹… ì„¤ì •"""
    log_format = "%(asctime)s [%(levelname)s] %(message)s"
    log_date_format = "%Y-%m-%d %H:%M:%S"

    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format=log_format,
        datefmt=log_date_format,
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(
                PROJECT_ROOT / "outputs" / "analysis.log",
                encoding="utf-8",
            ),
        ],
    )


logger = logging.getLogger(__name__)


def run_daily_analysis(date: str = None) -> dict:
    """
    ì¼ì¼ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

    í”„ë¡¬í”„íŠ¸ ì²´ì´ë‹ íë¦„:
    Step 1: pykrxë¡œ ì›ì‹œ ë°ì´í„° ìˆ˜ì§‘ (Python)
    Step 2: í”„ë¡¬í”„íŠ¸ 1 â†’ í•„í„°ë§ ë° ì •ë¦¬ (AI)
    Step 3: pykrx + OpenDartë¡œ ì¬ë¬´ì§€í‘œ ìˆ˜ì§‘ (Python)
    Step 4: í”„ë¡¬í”„íŠ¸ 2 â†’ ì¬ë¬´ì§€í‘œ ë¶„ì„ (AI)
    Step 5: ë„¤ì´ë²„ ê²€ìƒ‰ APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ (Python)
    Step 6: í”„ë¡¬í”„íŠ¸ 3 â†’ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ (AI)
    Step 7: í”„ë¡¬í”„íŠ¸ 4 â†’ ì¢…í•© ë¶„ì„ (AI)
    Step 8: í”„ë¡¬í”„íŠ¸ 5 â†’ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± (AI)

    Args:
        date: ë¶„ì„ ë‚ ì§œ (YYYYMMDD, ê¸°ë³¸ê°’: ìµœê·¼ ê±°ë˜ì¼)

    Returns:
        dict: ë¶„ì„ ê²°ê³¼ ë° ì €ì¥ íŒŒì¼ ê²½ë¡œ
    """
    start_time = datetime.now()
    logger.info("=" * 60)
    logger.info("ğŸš€ í•œêµ­ ì£¼ì‹ ì‹œì¥ ì¼ì¼ ë¶„ì„ ì‹œì‘")
    logger.info("=" * 60)

    # ëª¨ë“ˆ ì´ˆê¸°í™”
    data_collector = StockDataCollector()
    news_collector = NewsCollector()
    report_generator = ReportGenerator()

    # ===== Step 1 & 3: ë°ì´í„° ìˆ˜ì§‘ (Python) =====
    logger.info("\nğŸ“¥ [Step 1/3] ì‹œì¥ ë°ì´í„° ë° ì¬ë¬´ì§€í‘œ ìˆ˜ì§‘...")
    collected_data = data_collector.collect_all_data(date)
    analysis_date = collected_data["ê¸°ì¤€ì¼"]

    filtered_count = collected_data["í•„í„°ë§_ê²°ê³¼"]["í•„í„°ë§_ì¢…ëª©ìˆ˜"]
    logger.info(f"  â†’ í•„í„°ë§ ì¢…ëª© ìˆ˜: {filtered_count}ê°œ")

    if filtered_count == 0:
        logger.warning("âš ï¸ í•„í„°ë§ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        logger.info("  â†’ ì¡°ê±´ì„ ì™„í™”í•˜ì—¬ ì¬ì‹œë„í•˜ê±°ë‚˜ ë‚ ì§œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return {
            "status": "no_data",
            "date": analysis_date,
            "message": "í•„í„°ë§ ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.",
        }

    # ===== Step 5: ë‰´ìŠ¤ ìˆ˜ì§‘ (Python) =====
    logger.info("\nğŸ“° [Step 5] ì¢…ëª©ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘...")
    stocks = collected_data["í•„í„°ë§_ê²°ê³¼"]["ì¢…ëª©_ë¦¬ìŠ¤íŠ¸"]
    all_news = news_collector.collect_all_stock_news(stocks, num_articles_per_stock=10)
    collected_data["ë‰´ìŠ¤_ë°ì´í„°"] = all_news
    logger.info(
        f"  â†’ ì´ {sum(n['ìˆ˜ì§‘_ë‰´ìŠ¤ìˆ˜'] for n in all_news)}ê±´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ"
    )

    # ===== Step 2/4/6/7/8: AI ë¶„ì„ (í”„ë¡¬í”„íŠ¸ ì²´ì´ë‹) =====
    logger.info("\nğŸ¤– [Step 2/4/6/7/8] AI í”„ë¡¬í”„íŠ¸ ì²´ì´ë‹ ë¶„ì„ ì‹œì‘...")
    try:
        ai_analyzer = AIAnalyzer()
        analysis_result = ai_analyzer.run_full_analysis(collected_data)
    except ValueError as e:
        logger.error(f"âŒ AI ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.info("  â†’ API í‚¤ ì—†ì´ ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼ë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
        analysis_result = {
            "filtered_analysis": collected_data["í•„í„°ë§_ê²°ê³¼"],
            "fundamental_analysis": collected_data["ì¬ë¬´ì§€í‘œ"],
            "news_analysis": [],
            "comprehensive_analysis": {},
            "report_markdown": "",
        }
    except Exception as e:
        logger.error(f"âŒ AI ë¶„ì„ ì‹¤íŒ¨: {e}")
        analysis_result = {
            "filtered_analysis": collected_data["í•„í„°ë§_ê²°ê³¼"],
            "fundamental_analysis": collected_data["ì¬ë¬´ì§€í‘œ"],
            "news_analysis": [],
            "comprehensive_analysis": {},
            "report_markdown": "",
        }

    # ===== ê²°ê³¼ ì €ì¥ =====
    logger.info("\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
    saved_files = report_generator.save_full_output(analysis_result, analysis_date)

    # ìˆ˜ì§‘ ì›ì‹œ ë°ì´í„°ë„ ë³„ë„ ì €ì¥
    raw_data_path = report_generator.save_json_data(
        collected_data, analysis_date, "raw_collected"
    )
    saved_files["raw_data"] = raw_data_path

    elapsed = (datetime.now() - start_time).total_seconds()
    logger.info("\n" + "=" * 60)
    logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ! (ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ)")
    logger.info(f"ğŸ“„ ë¦¬í¬íŠ¸: {saved_files.get('markdown', 'N/A')}")
    logger.info(f"ğŸ“Š ë°ì´í„°: {saved_files.get('json', 'N/A')}")
    logger.info("=" * 60)

    return {
        "status": "success",
        "date": analysis_date,
        "filtered_count": filtered_count,
        "saved_files": saved_files,
        "elapsed_seconds": elapsed,
    }


def main():
    """CLI ì—”íŠ¸ë¦¬í¬ì¸íŠ¸"""
    parser = argparse.ArgumentParser(
        description="í•œêµ­ ì£¼ì‹ ì‹œì¥ AI ë¶„ì„ ì‹œìŠ¤í…œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python main.py                    ì¦‰ì‹œ 1íšŒ ë¶„ì„ ì‹¤í–‰
  python main.py --date 20260206    íŠ¹ì • ë‚ ì§œ ë¶„ì„
  python main.py --schedule         ë§¤ì¼ ìë™ ì‹¤í–‰ (ê¸°ë³¸ 15:40)
  python main.py --schedule --time 16:00  ë§¤ì¼ 16:00ì— ìë™ ì‹¤í–‰
  streamlit run app.py              Streamlit ëŒ€ì‹œë³´ë“œ ì‹¤í–‰
        """,
    )
    parser.add_argument(
        "--date",
        type=str,
        default=None,
        help="ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ (YYYYMMDD í˜•ì‹, ê¸°ë³¸: ìµœê·¼ ê±°ë˜ì¼)",
    )
    parser.add_argument(
        "--schedule",
        action="store_true",
        help="ìŠ¤ì¼€ì¤„ ëª¨ë“œë¡œ ì‹¤í–‰ (ë§¤ì¼ ì§€ì • ì‹œê°ì— ìë™ ë¶„ì„)",
    )
    parser.add_argument(
        "--time",
        type=str,
        default="15:40",
        help="ìŠ¤ì¼€ì¤„ ì‹¤í–‰ ì‹œê° (HH:MM í˜•ì‹, ê¸°ë³¸: 15:40)",
    )
    parser.add_argument(
        "--log-level",
        type=str,
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="ë¡œê¹… ë ˆë²¨ (ê¸°ë³¸: INFO)",
    )

    args = parser.parse_args()
    setup_logging(args.log_level)

    if args.schedule:
        # ìŠ¤ì¼€ì¤„ ëª¨ë“œ
        scheduler = AnalysisScheduler(lambda: run_daily_analysis(args.date))
        scheduler.start(run_time=args.time)
    else:
        # ì¦‰ì‹œ ì‹¤í–‰ ëª¨ë“œ
        result = run_daily_analysis(args.date)
        if result["status"] == "success":
            print(f"\nâœ… ë¶„ì„ ì™„ë£Œ: {result['filtered_count']}ê°œ ì¢…ëª© ë¶„ì„ë¨")
            print(f"ğŸ“„ ë¦¬í¬íŠ¸: {result['saved_files'].get('markdown', 'N/A')}")
        else:
            print(f"\nâš ï¸ {result.get('message', 'ë¶„ì„ ì‹¤íŒ¨')}")


if __name__ == "__main__":
    main()
